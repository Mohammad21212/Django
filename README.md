
---

# پروژه API اخبار و جمع‌آوری اخبار با Scrapy

## توضیحات کلی

این پروژه شامل دو بخش اصلی است:
1. **چالش اول**: پیاده‌سازی یک API با استفاده از Django REST Framework که اخبار را مدیریت می‌کند و امکاناتی مثل افزودن، جستجو و فیلتر اخبار را فراهم می‌کند.
2. **چالش دوم**: جمع‌آوری اخبار از وب‌سایت‌های مختلف با استفاده از Scrapy و ذخیره‌سازی آن‌ها در پایگاه داده Django.

---

## بخش اول: ایجاد API اخبار با Django REST Framework

### مرحله 1: نصب و راه‌اندازی پروژه Django

ابتدا باید یک پروژه Django ایجاد شود. مراحل زیر انجام می‌شود:
- یک محیط مجازی Python ایجاد و فعال می‌گردد.
- Django و Django REST Framework نصب می‌شود.
- یک پروژه Django با دستور `django-admin startproject` ایجاد می‌شود.

### مرحله 2: ایجاد اپلیکیشن `news`

- یک اپلیکیشن جدید به نام `news` برای مدیریت اخبار ایجاد می‌شود.
- اپلیکیشن `news` به لیست `INSTALLED_APPS` در فایل `settings.py` اضافه می‌شود.
- یک مدل برای اخبار در فایل `models.py` ایجاد می‌شود که شامل فیلدهای `title`، `content`، `tag` و `source` است.

### مرحله 3: ایجاد API با Django REST Framework

- سریالایزرهای مربوط به مدل `News` ایجاد می‌گردد تا داده‌ها به فرمت JSON تبدیل شوند.
- ویوهای مربوط به API ایجاد می‌شود که شامل قابلیت‌هایی مثل مشاهده لیست اخبار، فیلتر بر اساس تگ و جستجوی کلمات کلیدی است.
- از `DjangoFilterBackend` برای اضافه کردن فیلتر بر اساس تگ استفاده می‌شود.
- قابلیت جستجوی کلمات کلیدی در عنوان و محتوای اخبار با استفاده از `SearchFilter` فراهم می‌شود.
- تست‌های واحد برای بررسی صحت عملکرد API نوشته می‌شود.

### مرحله 4: تست API

- سرور Django با دستور `python manage.py runserver` اجرا می‌گردد.
- از طریق ابزارهایی مثل Postman یا curl، API تست می‌شود.
  - برای مثال، لیست اخبار از مسیر `/api/news/` دریافت می‌شود.
  - پارامترهای جستجو در URL وارد می‌شود تا فیلترها و جستجوها تست شوند.

---

## بخش دوم: جمع‌آوری اخبار با Scrapy و ذخیره آن‌ها در دیتابیس Django

### مرحله 1: نصب و راه‌اندازی Scrapy

- کتابخانه Scrapy نصب می‌شود.
- یک اسپایدر Scrapy ایجاد می‌شود که از وب‌سایت‌های مشخص شده اخبار جمع‌آوری کند.

### مرحله 2: جمع‌آوری داده‌ها

- URLهای مختلفی که قرار است از آن‌ها اخبار جمع‌آوری شود در لیست `start_urls` تعریف می‌شوند.
- اسپایدر Scrapy به هر صفحه مراجعه کرده و اطلاعات مربوط به عنوان، محتوا، تگ و منبع خبر استخراج می‌شود.
- پس از استخراج داده‌ها، آن‌ها در لیستی ذخیره می‌شوند.

### مرحله 3: ادغام Scrapy با Django

- برای اینکه Scrapy داده‌ها را مستقیماً به دیتابیس Django اضافه کند، ابتدا تنظیمات Django به اسکریپت Scrapy اضافه می‌شود.
- به جای ذخیره‌سازی داده‌ها در فایل‌های Excel، داده‌ها مستقیماً به مدل `News` در Django ارسال می‌شوند.
- اسکریپت Scrapy به عنوان یک اسکریپت قابل اجرا از طریق Django Extensions تنظیم می‌شود.

### مرحله 4: اجرای Scrapy و ذخیره داده‌ها در دیتابیس

- اسکریپت Scrapy با استفاده از دستور `python manage.py runscript` اجرا می‌شود.
- Scrapy به وب‌سایت‌ها مراجعه می‌کند، داده‌ها را جمع‌آوری کرده و آن‌ها را در دیتابیس ذخیره می‌کند.

---

## نکات مهم

- تنظیمات پایگاه داده در فایل `settings.py` باید به درستی انجام شود.
  - اگر از SQLite استفاده می‌شود، تنظیمات پیش‌فرض کافی است.
  - در صورت استفاده از PostgreSQL یا MySQL، تنظیمات مربوطه در `DATABASES` انجام می‌شود.
  
- برای اجرای Scrapy از Django Extensions استفاده می‌شود، بنابراین اطمینان حاصل شود که این کتابخانه نصب و پیکربندی شده است.

- تست‌های واحد برای بخش API نوشته شده‌اند تا از صحت عملکرد جستجو و فیلترها اطمینان حاصل شود.

---

## نحوه اجرا

### مرحله 1: اجرای API

- ابتدا پایگاه داده با استفاده از دستور `python manage.py migrate` آماده می‌شود.
- سپس سرور Django با دستور `python manage.py runserver` اجرا می‌شود.
- از طریق Postman یا مرورگر، به API متصل شده و عملیات‌های مورد نظر (افزودن، جستجو، ویرایش و حذف) انجام می‌شود.

### مرحله 2: اجرای Scrapy

- دستور `python manage.py runscript scraper_script` اجرا می‌شود تا Scrapy شروع به جمع‌آوری اخبار کند.
- Scrapy داده‌ها را به صورت مستقیم در مدل `News` ذخیره خواهد کرد.

---

