# پروژه tech_news

## توضیحات کلی

این پروژه شامل سه بخش اصلی است:
1. **چالش اول**: پیاده‌سازی یک API با استفاده از Django REST Framework که اخبار را مدیریت می‌کند و امکاناتی مثل افزودن، جستجو و فیلتر اخبار را فراهم می‌کند.
2. **چالش دوم**: جمع‌آوری اخبار از وب‌سایت‌های مختلف با استفاده از Scrapy و ذخیره‌سازی آن‌ها در پایگاه داده Django.
3. **چالش سوم**: جمع‌آوری اخبار از وب‌سایت Reuters با استفاده از Scrapy و Selenium و ذخیره‌سازی داده‌ها در فایل CSV.

---

## بخش اول: ایجاد API اخبار با Django REST Framework

### مرحله 1: نصب و راه‌اندازی پروژه Django

ابتدا باید یک پروژه Django ایجاد شود. مراحل زیر انجام می‌شود:
- یک محیط مجازی Python ایجاد و فعال می‌گردد.
- سپس Django و Django REST Framework نصب می‌شود.
- یک پروژه Django با دستور `django-admin startproject` ایجاد می‌شود.

### مرحله 2: ایجاد اپلیکیشن `news`

- یک اپلیکیشن جدید به نام `news` برای مدیریت اخبار ایجاد می‌شود.
- اپلیکیشن `news` به لیست `INSTALLED_APPS` در فایل `settings.py` اضافه می‌شود.
- یک مدل برای اخبار در فایل `models.py` ایجاد می‌شود که شامل فیلدهای `title`، `content`، `tag` و `source` است.

### مرحله 3: ایجاد API با Django REST Framework

- سریالایزرهای مربوط به مدل `News` ایجاد می‌گردد تا داده‌ها به فرمت JSON تبدیل شوند.
- ویوهای مربوط به API ایجاد می‌شود که شامل قابلیت‌هایی مثل مشاهده لیست اخبار، فیلتر بر اساس تگ و جستجوی کلمات کلیدی است.
- از `DjangoFilterBackend` برای اضافه کردن فیلتر بر اساس تگ استفاده می‌شود.
- قابلیت جستجوی کلمات کلیدی در عنوان و محتوای اخبار با استفاده از `SearchFilter` فراهم می‌شود.
- تست‌های واحد برای بررسی صحت عملکرد API نوشته می‌شود.

### مرحله 4: تست API

- سرور Django با دستور `python manage.py runserver` اجرا می‌گردد.
  - با یونیت تست می شود.
  - برای مثال، لیست اخبار از مسیر `/api/news/` دریافت می‌شود.
  - پارامترهای جستجو در URL وارد می‌شود تا فیلترها و جستجوها تست شوند.

---

## بخش دوم: جمع‌آوری اخبار با Scrapy و ذخیره آن‌ها در دیتابیس Django

### مرحله 1: نصب و راه‌اندازی Scrapy

- کتابخانه Scrapy نصب می‌شود.
- یک اسپایدر Scrapy ایجاد می‌شود که از وب‌سایت‌های مشخص شده اخبار جمع‌آوری کند.

### مرحله 2: جمع‌آوری داده‌ها

- ابتدا URLهای مختلفی که قرار است از آن‌ها اخبار جمع‌آوری شود در لیست `start_urls` تعریف می‌شوند.
- اسپایدر Scrapy به هر صفحه مراجعه کرده و اطلاعات مربوط به عنوان، محتوا، تگ و منبع خبر استخراج می‌شود.
- پس از استخراج داده‌ها، آن‌ها در لیستی ذخیره می‌شوند.

### مرحله 3: ادغام Scrapy با Django

- برای اینکه Scrapy داده‌ها را مستقیماً به دیتابیس Django اضافه کند، ابتدا تنظیمات Django به اسکریپت Scrapy اضافه می‌شود.
- به جای ذخیره‌سازی داده‌ها در فایل‌های Excel، داده‌ها مستقیماً به مدل `News` در Django ارسال می‌شوند.
- اسکریپت Scrapy به عنوان یک اسکریپت قابل اجرا از طریق Django Extensions تنظیم می‌شود.

### مرحله 4: اجرای Scrapy و ذخیره داده‌ها در دیتابیس

- اسکریپت Scrapy با استفاده از دستور `python manage.py runscript` اجرا می‌شود.
- سپس Scrapy به وب‌سایت‌ها مراجعه می‌کند، داده‌ها را جمع‌آوری کرده و آن‌ها را در دیتابیس ذخیره می‌کند.

---

## بخش سوم: جمع‌آوری اخبار با استفاده از Crawl و Selenium 

در چالش سوم، از **Scrapy** و **Selenium** برای جمع‌آوری اخبار از وب‌سایت Reuters استفاده می‌شود. داده‌های مربوط به هر خبر شامل عنوان، نویسنده، منبع، تگ‌ها و متن خبر استخراج شده و به جای ذخیره در پایگاه داده، در یک فایل CSV ذخیره می‌شوند.

### مرحله 1: نصب و راه‌اندازی Scrapy و Selenium

- ابتدا **Selenium** برای تعامل با وب‌سایت‌ها و استخراج لینک‌ها استفاده می‌شود.
- سپس **Scrapy** برای پردازش صفحات و جمع‌آوری محتوای اخبار استفاده می‌شود.
- کتابخانه‌های مورد نیاز مانند `selenium`، `scrapy` و `webdriver_manager` نصب می‌شوند.

### مرحله 2: اجرای اسکریپت Scrapy و Selenium

- با استفاده از Selenium، لینک‌های مربوط به اخبار از صفحات جستجوی سایت Reuters استخراج می‌شوند.
- سپس Scrapy از این لینک‌ها برای پردازش و استخراج داده‌های مورد نظر استفاده می‌کند.
- داده‌های جمع‌آوری شده شامل عنوان خبر، نویسنده، منبع، تگ‌ها و متن خبر هستند.

### مرحله 3: ذخیره‌سازی داده‌ها در فایل CSV

- داده‌های استخراج شده از Scrapy در یک فایل CSV ذخیره می‌شوند. این فایل در همان پوشه `scripts` پروژه ذخیره خواهد شد.

### مرحله 4: اجرای اسکریپت از طریق Django

- اسکریپت به عنوان یک اسکریپت قابل اجرا از طریق Django Extensions پیکربندی شده است.
- با استفاده از دستور زیر، اسکریپت Scrapy اجرا می‌شود:

  ```bash
  python manage.py runscript reuters_spider
  ```

- پس از اجرا، داده‌ها به صورت خودکار در فایل CSV ذخیره خواهند شد.

### نکات مهم

- اسکریپت Scrapy با استفاده از Selenium برای تعامل با سایت و استخراج لینک‌ها به کار می‌رود.
- داده‌ها به جای ذخیره در دیتابیس، در فایل CSV ذخیره می‌شوند تا به راحتی مورد استفاده قرار گیرند.

---

## نکات مهم

- تنظیمات پایگاه داده در فایل `settings.py` باید به درستی انجام شود.
  - اگر از SQLite استفاده می‌شود، تنظیمات پیش‌فرض کافی است.
  - در صورت استفاده از PostgreSQL یا MySQL، تنظیمات مربوطه در `DATABASES` انجام می‌شود.
  
- برای اجرای Scrapy از Django Extensions استفاده می‌شود، بنابراین اطمینان حاصل شود که این کتابخانه نصب و پیکربندی شده است.

- تست‌های واحد برای بخش API نوشته شده‌اند تا از صحت عملکرد جستجو و فیلترها اطمینان حاصل شود.

---

## نحوه اجرا

### مرحله 1: اجرای API

- ابتدا پایگاه داده با استفاده از دستور `python manage.py migrate` آماده می‌شود.
- سپس سرور Django با دستور `python manage.py runserver` اجرا می‌شود.
- از طریق Postman یا مرورگر، به API متصل شده و عملیات‌های مورد نظر (افزودن، جستجو، ویرایش و حذف) انجام می‌شود.

### مرحله 2: اجرای Scrapy برای چالش 2

- دستور `python manage.py runscript scraper_script` اجرا می‌شود تا Scrapy شروع به جمع‌آوری اخبار کند.
- سپس Scrapy داده‌ها را به صورت مستقیم در مدل `News` ذخیره خواهد کرد.

### مرحله 3: اجرای Scrapy برای چالش 3

- دستور زیر برای اجرای Scrapy و Selenium اجرا می‌شود:

   ```bash
   python manage.py runscript reuters_spider
   ```

- داده‌های جمع‌آوری شده در فایل `gold_commodity_news.csv` در پوشه `scripts` ذخیره می‌شوند.

---

این فایل README شامل توضیحات کامل هر سه چالش است و نحوه اجرا و نصب پروژه را به خوبی پوشش می‌دهد.
